{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfDL5xkoKh4tnL/1KhL8RS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isomjd-code/latin-courthand-correction/blob/main/latin_courthand_llm_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0jigCeGSWaB",
        "outputId": "387dd769-2b71-4ecd-e411-a63483a27d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required libraries for Google Colab...\n",
            "Installation complete.\n",
            "==============================================================\n",
            "          STARTING TRANSKRIBUS CORRECTION PROCESS\n",
            "          Document ID: 9310076 in Collection: 1957043\n",
            "==============================================================\n",
            "Anthropic client initialized.\n",
            "Authenticating with Transkribus as user: isomjd@gmail.com...\n",
            "Transkribus authentication successful.\n",
            "\n",
            "--- [Step 1/7] Fetching Page Details from Transkribus ---\n",
            "Getting page details for document 9310076 (page index 0)...\n",
            "DEBUG: Found 'trpPage' key in API response. Parsing list from there.\n",
            "Found page details: Page Number=1, Image URL found.\n",
            "\n",
            "--- [Step 2/7] Downloading Document Data ---\n",
            "Downloading PAGE XML for doc 9310076, page 1...\n",
            "PAGE XML downloaded successfully to temp_page_for_correction.xml\n",
            "Downloading document image...\n",
            "Image downloaded successfully (349.7 KB).\n",
            "\n",
            "--- [Step 3/7] Parsing Local Data and Fetching Entities ---\n",
            "Parsing XML file: temp_page_for_correction.xml...\n",
            "Successfully parsed XML. Found 37 text lines.\n",
            "Document Title: IMG_0276.jpg\n",
            "Warning: Could not parse side/number from document title. Skipping entity fetch.\n",
            "\n",
            "--- [Step 4/7] Processing 37 lines in chunks of 10 ---\n",
            "\n",
            "  Processing Chunk 1/4 (Lines 1-10)...\n",
            "    - Preparing image chunk...\n",
            "Warning: DejaVuSans.ttf not found. Using default font.\n",
            "    - Saved numbered chunk image to temp_chunk_images/chunk_1.jpg\n",
            "    - Making LLM call for Run A...\n",
            "    - Run A successful.\n",
            "    - Making LLM call for Run B...\n",
            "    - Run B successful.\n",
            "\n",
            "  Processing Chunk 2/4 (Lines 11-20)...\n",
            "    - Preparing image chunk...\n",
            "Warning: DejaVuSans.ttf not found. Using default font.\n",
            "    - Saved numbered chunk image to temp_chunk_images/chunk_2.jpg\n",
            "    - Making LLM call for Run A...\n",
            "    - Run A successful.\n",
            "    - Making LLM call for Run B...\n",
            "    - Run B successful.\n",
            "\n",
            "  Processing Chunk 3/4 (Lines 21-30)...\n",
            "    - Preparing image chunk...\n",
            "Warning: DejaVuSans.ttf not found. Using default font.\n",
            "    - Saved numbered chunk image to temp_chunk_images/chunk_3.jpg\n",
            "    - Making LLM call for Run A...\n",
            "    - Run A successful.\n",
            "    - Making LLM call for Run B...\n",
            "    - Run B successful.\n",
            "\n",
            "  Processing Chunk 4/4 (Lines 31-37)...\n",
            "    - Preparing image chunk...\n",
            "Warning: DejaVuSans.ttf not found. Using default font.\n",
            "    - Saved numbered chunk image to temp_chunk_images/chunk_4.jpg\n",
            "    - Making LLM call for Run A...\n",
            "    - Run A successful.\n",
            "    - Making LLM call for Run B...\n",
            "    - Run B successful.\n",
            "\n",
            "--- [Step 5/7] Analyzing LLM Results and Flagging Uncertainties ---\n",
            "Analysis complete. Found 13 lines with high CER to be flagged as 'unclear'.\n",
            "\n",
            "--- [Step 6/7] Generating Final Corrected XML ---\n",
            "Modifying XML with corrected transcriptions...\n",
            "XML modification complete. Lines updated: 21. Lines flagged as uncertain: 13.\n",
            "\n",
            "--- [Step 7/7] Uploading Final XML to Transkribus ---\n",
            "Uploading corrected XML to Transkribus for doc 9310076, page 1...\n",
            "✅ Successfully updated page in Transkribus!\n",
            "\n",
            "--- Cleaning up temporary files... ---\n",
            "Removed temp_page_for_correction.xml\n",
            "\n",
            "==============================================================\n",
            "            PROCESS COMPLETE\n",
            "            Total execution time: 62.04 seconds.\n",
            "==============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "#               Claude-Based Transkribus Correction Script (Single Document)\n",
        "# ==============================================================================\n",
        "#\n",
        "# DESCRIPTION:\n",
        "# This script is designed to be run in a Google Colab notebook. It takes a single\n",
        "# document from a Transkribus collection, processes it line-by-line in chunks,\n",
        "# and uses the Anthropic Claude 3 Sonnet model to correct the transcription.\n",
        "#\n",
        "# It performs a \"two-call\" correction for each chunk of text:\n",
        "# 1. It sends the image chunk and HTR text to Claude twice to get two independent\n",
        "#    transcriptions (Run A and Run B).\n",
        "# 2. It compares the two Claude versions. If they differ significantly, the line\n",
        "#    is flagged as \"unclear\" in the final XML.\n",
        "# 3. It selects the best transcription for each line by comparing Claude's outputs\n",
        "#    to the original HTR text and choosing the one with the lowest Character\n",
        "#    Error Rate (CER).\n",
        "# 4. It writes the corrected text and metadata back to Transkribus, overwriting\n",
        "#    the latest version of the page transcription.\n",
        "#\n",
        "# HOW TO USE IN GOOGLE COLAB:\n",
        "# 1. In the left sidebar, click the key icon and create secrets for:\n",
        "#    - TRANKRIBUS_USER\n",
        "#    - TRANKRIBUS_PASSWORD\n",
        "#    - anthropic_key\n",
        "# 2. Paste this entire script into a single cell in a new Colab notebook.\n",
        "# 3. Fill in the document details in the \"USER CONFIGURATION\" section below.\n",
        "# 4. Run the cell.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Step 0: Install Dependencies (only runs if in a Colab-like environment) ---\n",
        "try:\n",
        "    import google.colab\n",
        "    print(\"Installing required libraries for Google Colab...\")\n",
        "    # Use -q for a quieter installation\n",
        "    !pip install -q anthropic python-dotenv beautifulsoup4 lxml python-levenshtein Pillow requests numpy\n",
        "    print(\"Installation complete.\")\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab. Assuming libraries are already installed.\")\n",
        "\n",
        "# --- Library Imports ---\n",
        "import xml.etree.ElementTree as ET\n",
        "import requests\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import anthropic\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import Levenshtein\n",
        "import datetime\n",
        "import math\n",
        "import numpy as np\n",
        "import base64\n",
        "import re\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "# Import userdata for secrets management in Colab\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except ImportError:\n",
        "    userdata = None # Will cause an error later if not in Colab, which is intended.\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                           1. USER CONFIGURATION\n",
        "# ==============================================================================\n",
        "# --- Transkribus Credentials (from Colab Secrets) ---\n",
        "# Ensure you have set these secrets in your Colab environment (left sidebar, key icon)\n",
        "if userdata:\n",
        "    TRANKRIBUS_USERNAME = userdata.get('TRANKRIBUS_USER')\n",
        "    TRANKRIBUS_PASSWORD = userdata.get('TRANKRIBUS_PASSWORD')\n",
        "    ANTHROPIC_API_KEY = userdata.get('anthropic_key')\n",
        "else:\n",
        "    # Fallback for local execution (not recommended for sharing)\n",
        "    TRANKRIBUS_USERNAME = \"YOUR_LOCAL_EMAIL\"\n",
        "    TRANKRIBUS_PASSWORD = \"YOUR_LOCAL_PASSWORD\"\n",
        "    ANTHROPIC_API_KEY = \"YOUR_LOCAL_API_KEY\"\n",
        "\n",
        "# --- Document to Process ---\n",
        "COLLECTION_ID = 1957043  # <<< --- YOUR COLLECTION ID --- >>>\n",
        "DOCUMENT_ID = 9310076  # <<< --- THE SPECIFIC DOCUMENT ID YOU WANT TO PROCESS --- >>>\n",
        "\n",
        "# --- LLM & Processing Configuration ---\n",
        "ANTHROPIC_MODEL_NAME = \"claude-3-7-sonnet-20250219\" # Or \"claude-3-opus-20240229\", \"claude-3-haiku-20240307\"\n",
        "LLM_TEMPERATURE = 0.3      # Controls the randomness of the LLM. 0.3 is a good balance for this task.\n",
        "ANTHROPIC_MAX_TOKENS = 4096  # Max tokens for Claude's response per chunk.\n",
        "CHUNK_SIZE = 10            # Number of lines to process in each call to the LLM.\n",
        "BBOX_PADDING = 40          # Pixels to add around text lines when cropping the image.\n",
        "\n",
        "# --- Thresholds for Analysis ---\n",
        "CER_THRESHOLD = 0.05  # CER between Claude's two versions. If higher, the line is marked \"unclear\".\n",
        "LLM_HTR_CER_THRESHOLD = 0.05 # Max CER between any LLM output and HTR to be considered 'certain'\n",
        "\n",
        "# --- AALT Index Configuration (for fetching named entities) ---\n",
        "AALT_INDEX_URL = \"https://waalt.uh.edu/index.php/KB27/795\"\n",
        "\n",
        "# --- File/Directory Configuration ---\n",
        "TEMP_XML_FILENAME = \"temp_page_for_correction.xml\" # Temporary file to store downloaded XML.\n",
        "TEMP_CHUNK_IMAGE_DIR = \"temp_chunk_images\" # Directory to save temporary images for debugging.\n",
        "\n",
        "# ==============================================================================\n",
        "#                       2. SYSTEM PROMPT FOR THE LLM\n",
        "# ==============================================================================\n",
        "# This is the detailed set of instructions given to the Claude model.\n",
        "# It defines the expert persona, transcription rules, and output format.\n",
        "# This prompt is critical to the quality of the output.\n",
        "\n",
        "SYSTEM_PROMPT_BASE = \"\"\"\n",
        "YOUR ROLE: Medieval Latin Paleography Expert\n",
        "You are transcribing Latin legal documents from the English Court of Common Pleas for the `abbreviated_latin_lines` output. Strictly follow these rules:\n",
        "\n",
        "---\n",
        "**A. SOURCE PRIORITIZATION & HTR USAGE**\n",
        "1.  **Source Priority:** Use sources in this order: 1. Document Image (Absolute authority for form/content), 2. HTR Transcription (Word ID aid ONLY - ignore its expansion status), 3. Named Entity List (Reference), 4. Your Knowledge (Lowest). **# ADDED: Always prioritize the Document Image over the HTR and Named Entity List if there is a conflict in spelling, capitalization, or abbreviation form.**\n",
        "2.  **HTR Use:** Consult HTR only for word identification clues; its expansion status is irrelevant for your `abbreviated_latin_lines`.\n",
        "\n",
        "---\n",
        "**B. CORE PRINCIPLE: IMAGE AUTHORITY & **ABSOLUTE LINE INTEGRITY** (Non-Negotiable)**\n",
        "3.  **Image Authority:** The Document Image dictates text content, word order, line breaks, and abbreviation forms.\n",
        "4.  **Line Mapping & Structure (CRITICAL - Non-Negotiable):**\n",
        "    *   **Anchor to Red Numbers:** Your primary task is to map text directly to the **red numbers** visible on the image. Each red number (e.g., `10`) is located to the left of and slightly above the start of the specific baseline it identifies.\n",
        "    *   **JSON Key = Red Number:** The JSON key in your output (e.g., `\"10\"`) MUST correspond *exactly* to the red number (e.g., `10`) visible on the image next to the baseline whose text you are transcribing.\n",
        "    *   **First/Last Word Matching (CRITICAL):** Ensure the *first* word transcribed for JSON key `\"n\"` matches the *first* word visually associated with the baseline marked by the red number `n` on the image. Ensure the *last* word transcribed for JSON key `\"n\"` matches the *last* word visually associated with that same baseline.\n",
        "    *   **Preserve Line Content:** Ensure the text transcribed for JSON key `\"n\"` matches the text visually associated with the baseline marked by the red number `n` on the image.\n",
        "    *   **Strict Line Breaks (CRITICAL): DO NOT, under ANY circumstances, move words from the end of one numbered baseline to the beginning of the next, or vice-versa.** Each JSON line MUST correspond precisely and *only* to the text visually associated with its specific numbered baseline on the image.\n",
        "5.  **Transcription Scope (CRITICAL):**\n",
        "    *   **Transcribe ONLY Requested Numbered Lines:** Transcribe ONLY the text visibly associated with (typically above) the specific **numbered baselines requested for this chunk**. The requested line numbers are specified in the User Prompt (e.g., \"lines 10 to 19\").\n",
        "    *   **IGNORE Other Visible Lines:** Even if other text lines (or parts of lines) are visible in the image chunk (e.g., a line above the first requested number, or below the last requested number), you MUST **ignore** them completely if they do not correspond to one of the **red numbers within the requested range**. Do NOT transcribe text associated with unnumbered or out-of-range baselines.\n",
        "# ADDED Rule 5.1 to explicitly address omissions\n",
        "5.1 **Transcribe ALL Visible Text (CRITICAL - NO OMISSIONS):** You MUST transcribe **every** word, abbreviation, symbol, and mark visibly associated with the numbered baseline. **DO NOT OMIT ANY TEXT** present on the image for the requested line. Your transcription for line `n` must be a complete representation of the text on baseline `n`. Double-check you have captured the entire line content.\n",
        "6.  **Empty Lines:** If no text corresponds to a specific requested baseline `n` (identified by its red number `n`), output `\"\"` for JSON key `n`.\n",
        "7.  **Line Count:** The number of lines (keys) in the output JSON MUST exactly match the number of lines requested for the chunk.\n",
        "\n",
        "---\n",
        "**C. ABBREVIATION HANDLING (`abbreviated_latin_lines` - Visual Representation)**\n",
        "8.  **Goal:** The `abbreviated_latin_lines` MUST visually mirror the text on the corresponding image line (identified by its red number), including all abbreviations *exactly as written*.\n",
        "9.  **ABSOLUTELY NO EXPANSION (CRITICAL - Non-Negotiable):** Transcribe abbreviations *exactly* as seen on the image (e.g., `q'd`, `p'd'co`, `d'no`). Transcribe fully spelled-out words as seen (e.g., `quod`, `predictus`, `domino`). **NEVER** expand abbreviations shown on the image, nor abbreviate words spelled out on the image. Your primary goal for `abbreviated_latin_lines` is *visual fidelity* to the image line, not linguistic expansion or normalization. **# ADDED: Pay close attention to the *exact form* of the abbreviation as seen on the image. Transcribe *exactly* what you see.**\n",
        "    *   *More Examples:* Transcribe spelled-out `Et` as `Et`, NEVER `&`. Transcribe Tironian `&` as `&`, NEVER `Et`. Transcribe spelled-out `habeat` as `habeat`, NEVER `h'eat`. Transcribe abbreviated `p'cept'` as `p'cept'`, NEVER `p'ceptum`. Transcribe spelled-out `Robertum` as `Robertum`, NEVER `Rob'tum`. Transcribe abbreviated `Rob't` as `Rob't`, NEVER `Robertum`. Transcribe spelled-out `Willelmum` as `Willelmum`, NEVER `Will'm`. Transcribe abbreviated `Will'm` as `Will'm`, NEVER `Willelmum`. Transcribe `q'd` as `q'd`, NOT `quod`. Transcribe `p'd'co` as `p'd'co`, NOT `predicto`. Transcribe `uic'` as `uic'`, NOT `vicecomes`. Transcribe `pl'ito` as `pl'ito`, NOT `placito`.\n",
        "10. **Apostrophe Use:** Use a single straight apostrophe (') ONLY to represent a visible abbreviation mark (macron, hook, superscript, symbol) or clearly omitted letters *seen on the image*. Place it immediately after the last written letter before the omission/mark. Use universally for all mark types.\n",
        "\n",
        "---\n",
        "**D. SPECIFIC ABBREVIATIONS (Transcribe based on IMAGE evidence & Rules)**\n",
        "\n",
        "**CRITICAL ABBREVIATIONS (Strict Transcription - Follow Image & Rule C.9)**\n",
        "*   Tironian 'et' (&): MUST be transcribed as `&`. (**CRITICAL RULE D.11**)\n",
        "*   Spelled-out 'Et' or 'et': MUST be transcribed as `Et` or `et` (matching image case). (**CRITICAL RULE D.12**)\n",
        "*   'etcetera' abbr (&c): MUST be transcribed as `&c`. (Rule D.13)\n",
        "*   'quod' abbr (q w/ macron or similar mark): MUST be transcribed as `q'd`. (Rule D.14)\n",
        "*   'per'/'pro' abbr (crossed-p): MUST be transcribed as `p'`. (Rule D.16)\n",
        "*   'com'/'con' prefix abbr: MUST be transcribed as `com'`. (Rule D.18)\n",
        "*   'predictus' forms abbr: MUST be transcribed using `p'd'` prefix, e.g., `p'd'cus`, `p'd'co`, `p'd'ca`. (Rule D.26)\n",
        "*   'vicecomes/ti' abbr: MUST be transcribed as `vic'`. (Rule D.24)\n",
        "*   'nuper' abbr: MUST be transcribed as `nup'`. (Rule D.36)\n",
        "*   'apud' abbr: MUST be transcribed as `ap'd`. (Rule D.38)\n",
        "*   'super' abbr: MUST be transcribed as `sup'`. (Rule D.40)\n",
        "*   'placito' abbr: MUST be transcribed as `pl'ito`. (Rule D.44)\n",
        "*   'transgressio...' abbr: MUST be transcribed as `transgr'` (or `t'nsgr'` if seen). (Rule D.46)\n",
        "*   'ibidem' abbr: MUST be transcribed as `ib'm`. (Rule D.47)\n",
        "*   'scilicet' abbr: MUST be transcribed as `scil'` or `sc'l't` (matching image). (Rule D.49)\n",
        "*   'Willelmum/us' abbr: MUST be transcribed as `Will'm` (or specific form seen). (Rule D.51)\n",
        "*   'Ricardus/um' abbr: MUST be transcribed as `Ric'` (or specific form seen). (Rule D.53)\n",
        "*   'Thomas/am/e' abbr: MUST be transcribed as `Thom'` (or specific form seen). (Rule D.55)\n",
        "*   'Robertus/um' abbr: MUST be transcribed as `Rob't` (or specific form seen). (Rule D.57)\n",
        "\n",
        "*(Note: The following list provides standard forms. Your transcription MUST reflect the visual form on the image line, prioritizing the Critical Abbreviations above and Rule C.9.)*\n",
        "11. **`&` (CRITICAL):** Transcribe Tironian 'et' as `&`.\n",
        "12. **`et` / `Et` (CRITICAL):** Transcribe spelled-out `et` or `Et` exactly as written (matching case).\n",
        "# ADDED: Explicit negative constraint for & / Et\n",
        "**CRITICAL REMINDER:** **NEVER** transcribe a spelled-out `Et` or `et` on the image as `&`. **NEVER** transcribe a Tironian `&` on the image as `Et` or `et`. Follow the image exactly.\n",
        "13. **`&c`:** Transcribe 'etcetera' abbreviation (e.g., `&c`, `&c.`) as `&c`.\n",
        "14. **`q'd`:** Transcribe 'quod' abbreviation (e.g., `q` w/ macron or similar mark) as `q'd`.\n",
        "15. **`quod`:** Transcribe spelled-out `quod` as `quod`.\n",
        "16. **`p'`:** Transcribe 'per'/'pro' abbreviation (crossed-p) as `p'`.\n",
        "17. **`per`/`pro`:** Transcribe spelled-out `per` or `pro` as written.\n",
        "18. **`com'`:** Transcribe 'com'/'con' prefix abbreviation (macron/hook) as `com'`.\n",
        "19. **`comitatus`:** Transcribe spelled-out `comitatus`, `comiti`, etc., as written.\n",
        "20. **`d'ni`/`d'no`:** Transcribe 'dominus/i/o' abbreviation as `d'ni` or `d'no`.\n",
        "21. **`Dominus`:** Transcribe spelled-out `Dominus`, `domini`, `domino` as written (respect case).\n",
        "22. **`Ioh'es`/`Ioh'em`:** Transcribe 'Johannes/em' abbreviation as `Ioh'es` or `Ioh'em`.\n",
        "23. **`Iohannes`:** Transcribe spelled-out `Iohannes`, `Iohannem` as written.\n",
        "24. **`vic'`:** Transcribe 'vicecomes/ti' abbreviation as `vic'`.\n",
        "25. **`vicecomes`:** Transcribe spelled-out `vicecomes`, `vicecomiti` as written.\n",
        "26. **`p'd'`:** Transcribe 'predictus' forms abbreviation (e.g., `p'dcus`, `p'dco`) as `p'd'cus`, `p'd'co`, `p'd'ca`, etc.\n",
        "27. **`predictus`:** Transcribe spelled-out `predictus`, `predicto`, etc., as written.\n",
        "28. **`Reg'`:** Transcribe 'Regis/Rege/Regi' abbreviation as `Reg'`.\n",
        "29. **`Regis`/`Rege`/`Regi`:** Transcribe spelled-out `Regis`, `Rege`, `Regi` as written (capitalized).\n",
        "30. **`Angl'`:** Transcribe 'Anglie' abbreviation as `Angl'`.\n",
        "31. **`Anglie`:** Transcribe spelled-out `Anglie` as written.\n",
        "32. **`Westm'`:** Transcribe 'Westmonasterium' abbreviation as `Westm'`.\n",
        "33. **`Westmonasterium`:** Transcribe spelled-out `Westmonasterium` as written.\n",
        "34. **`attorn'`:** Transcribe 'attornatus/um' abbreviation as `attorn'`.\n",
        "35. **`attornatus`:** Transcribe spelled-out `attornatus`, `attornatum` as written.\n",
        "36. **`nup'`:** Transcribe 'nuper' abbreviation as `nup'`.\n",
        "37. **`nuper`:** Transcribe spelled-out `nuper` as written.\n",
        "38. **`ap'd`:** Transcribe 'apud' abbreviation as `ap'd`.\n",
        "39. **`apud`:** Transcribe spelled-out `apud` as written.\n",
        "40. **`sup'`:** Transcribe 'super' abbreviation as `sup'`.\n",
        "41. **`super`:** Transcribe spelled-out `super` as written.\n",
        "42. **`saluo'`:** Transcribe `saluo` with an abbreviation mark as `saluo'`.\n",
        "43. **`saluo`:** Transcribe spelled-out `saluo` as `saluo`.\n",
        "44. **`pl'ito`:** Transcribe 'placito' abbreviation as `pl'ito`.\n",
        "45. **`placito`:** Transcribe spelled-out `placito` as `placito`.\n",
        "46. **`transgr'`:** Transcribe 'transgressio...' abbreviation as `transgr'` (or `t'nsgr'` if seen).\n",
        "47. **`ib'm`:** Transcribe 'ibidem' abbreviation as `ib'm`.\n",
        "48. **`ibidem`:** Transcribe spelled-out `ibidem` as `ibidem`.\n",
        "49. **`scil'`/`sc'l't`:** Transcribe 'scilicet' abbreviation as `scil'` or `sc'l't` (matching image).\n",
        "50. **`scilicet`:** Transcribe spelled-out `scilicet` as `scilicet`.\n",
        "51. **`Will'm`:** Transcribe 'Willelmum/us' abbreviation as `Will'm` (or specific form seen).\n",
        "52. **`Willelmus`:** Transcribe spelled-out `Willelmus`, `Willelmum` as written.\n",
        "53. **`Ric'`:** Transcribe 'Ricardus/um' abbreviation as `Ric'` (or specific form seen).\n",
        "54. **`Ricardus`:** Transcribe spelled-out `Ricardus`, `Ricardum` as written.\n",
        "55. **`Thom'`:** Transcribe 'Thomas/am/e' abbreviation as `Thom'` (or specific form seen).\n",
        "56. **`Thomas`:** Transcribe spelled-out `Thomas`, `Thomam`, `Thome` as written.\n",
        "57. **`Rob't`:** Transcribe 'Robertus/um' abbreviation as `Rob't` (or specific form seen).\n",
        "58. **`Robertus`:** Transcribe spelled-out `Robertus`, `Robertum` as written.\n",
        "*(Note on Names: Pay close attention to whether names are abbreviated or fully spelled out on the image line and transcribe accordingly, following Rule C.9.)*\n",
        "\n",
        "---\n",
        "**E. REFERENCE EXAMPLES (Transcribe ONLY if seen on image)**\n",
        "*(This list shows common abbreviated phrases. Transcribe the exact form visible.)*\n",
        "*   absq' hoc q'd, accion'm ... h'ere non debet, ad cognosc', ad dampnum / Ad g've dampnu', ad faciend' & recipiend' q'd cur' ... cons', ad largum dimittatur, ad sectam, ad valenc', armig', assumpsit p' se ip'o, q'd attach' eu' / q'd att'o, bene et veru' est q'd, cal'pn' fu't, q'd cap'et eu' / capiatur / capiat, capiend' inde explec' ad valenc', ciuis et ..., clausum ... fregit, cl'icus, cons' est q'd, Et cont' pacem d'ni Regis, cuius dat' est die et Anno sup'dcis, de Com' in Com', de die in diem, de pl'ito q'd redd', de pl'ito detenc'o'is catallor', de pl'ito quare ... de novo fecit, defend' ius ... quando etc. / defend' vim et iniur' quando etc., p' defalt', deteriorat' sunt, diem p' ess' suos, die impetrac'onis b'ris originalis, domu' ... fregit, eat inde sine die, Et alia enormia etc., Et h'et etc., Et hoc paratus est verificare, Et hoc petit q'd inquiratur p' p'riam, Et ip'e non ven', Et p'd'cus ... similiter, Et q'd tale sit ius suum offert etc., Et saluo &c., Et totum etc., Et unde &c., Et vic' modo mand', exigat' eu' in Hustengo, q'd exigi fac' / exigifac', execut' test'i, expediens & necesse est, fenu' inde p'venient' ... cepit et asportavit, fil' et hered', gentilman' / Gentilman, gratis ... warr'izat, h'as d'ni Regis de p'donat'o utlagat'e, h'eat de t'ra ... ad valenc', husbondman, Idem dies dat' est, imp'p'm, in d'nico suo ut de feodo et iure, in m'ia / in misericordia, in Octab's s'ti Hillar' / a die pasche in xv dies / etc., in pp'ia p'sona sua, Io' / I'o, Ita q'd h'eret corpus eius hic / Ita q'd h'eat corpus eius hic, iuxta formam statuti, latitat vagat' et discurrit, p' legem t're, licet sepius requisit', manucep'unt, m'cator / m'cer, nich'il c'piat p' br'e suu', nich'il h'et / nichil habet, non est invent', non est p's / non s't p'sec', non obstante p'allegato, nondum reddidit, nup' de, nup' maiore', op' se iiij° die, ostens' si quid ... quare ... non debeat, pandoxator, p' aliqua p'allegata ab accio' sua ... p'cludi non deb', p' attornatu' suu', p' br'e d'ni Reg' de recto, p'c' fuit vic' / prec' est vic' / p'cept' fuit vic' / p'cept' est vic', pet' iud'm / petit iudiciu', pet' iud'm de br'i, pet' licenc' inde int'loquendi / vlt'ius pet' licenc', pet' recogn' fieri, pet' v'sus, pon' se in magnam assi'am d'ni Reg', postq'm sum' etc., p' int'esse suo, p'munt' fuit essond', p'muniant' p'usq'm, pbos & leg'les ho'ies, Et p'ferunt hic in cur' sc'ptum p'dcm, p'ut patet t'mio... Ro, p'ut p' b're et narracom' sup'pon', quare vi et armis, que fuit ux', queritur de, quiete de, quiet' & exon'at' a cur' dimittatur, quousq' &c., p' quos etc Et qui nec etc Ad recogn' etc Quia tam etc., recogn' de t'ris & catallis suis ad opus d'ni Regis levari, recup'et seisinam suam, scil't, scire fac', p' quoddam sc'ptum suu' obligator', s'viens, set in contemptum cur' recessit et defalt' fec', set sit in m'ia p'ro fa'l clam', si &c., sicut plur' / sicut plur'ies / sicut p'us, solempnit' exact', soluend', sub pena, q'd sum' eu' / q'd sum' eos, tenend' sibi et her' suis, tenentem p' warr' suam, t'pore pacis t'pore d'ni Reg' nunc, unde p'duc' sectam etc., ut ius et hereditatem suam, utlagat' / utlaget', utrum ip'e maius ius h'eat, uterq' eor' sum' est p', vic' non mis' br'e, voc' inde ad warr', yoman'\n",
        "\n",
        "---\n",
        "**F. LETTERFORMS & SPELLING**\n",
        "59. **CRITICAL LETTERFORMS (U/V & I/J):** **ALWAYS** use only 'u' or 'U' (never 'v' or 'V') and only 'i' or 'I' (never 'j' or 'J'). This applies to all words, including `uersus`, `uilla`, `ualenciam`, `iudicium`, `iuratus`, etc.\n",
        "60. **Long S:** Transcribe long 's' (ſ) as standard 's'.\n",
        "61. **Differentiation:** Carefully distinguish minims (n/u, m/in/ni/iu/ui), c/t, and f/s based on the image. **# ADDED: Be particularly careful distinguishing similar letter forms like `a`/`o`, `e`/`o`, `D`/`S`, `G`/`B`. Always verify against the image.**\n",
        "62. **Standard Letters:** Transcribe other letters to standard modern equivalents based on image form.\n",
        "63. **`uersus`/`sicut`:** Transcribe `uersus`/`u'sus` and `sicut`/`sic'` based on image form (abbreviated or full).\n",
        "\n",
        "---\n",
        "**G. CAPITALIZATION (Based on Image)**\n",
        "64. **Strict Following (CRITICAL):** Follow manuscript capitalization **EXACTLY** for **ALL** words.\n",
        "    *   *Example:* If the image shows `predictus`, transcribe `predictus`, NOT `Predictus`. If the image shows `Comes`, transcribe `Comes`, NOT `comes`. If the image shows `die`, transcribe `die`, NOT `Die`. If image shows `Anno`, transcribe `Anno`, NOT `anno`. Match the image case **exactly** for **all** words. # CHANGED: Strengthened rule and added more examples.\n",
        "65. **`Rex`/`Dominus`/`Regis`:** Capitalize `Rex`, `Dominus`, `Regis`, `Rege`, `Regi` if spelled out on image.\n",
        "66. **Nouns/Titles:** Capitalize proper nouns (people, places like `London'`, `Pasche`), titles/occupations (`Gentilman`, `yoman`, `clericus`) *only if capitalized on the image*. Retain abbreviation marks if present.\n",
        "\n",
        "---\n",
        "**H. NUMERALS & SYMBOLS**\n",
        "67. **Roman Numerals:** Transcribe Roman numerals exactly as they appear (e.g., `xij`, `xv`, `iiij`); do NOT convert to Arabic.\n",
        "68. **Paragraph Mark:** Transcribe the paragraph mark symbol as `¶` if present.\n",
        "\n",
        "---\n",
        "**I. WORKFLOW & VERIFICATION**\n",
        "69. **Image Focus:** Examine the image, identifying the **red numbers** marking the start of each requested baseline.\n",
        "70. **Structure Analysis:** Note paragraphs and line counts using the **red image numbers** and Named Entity List for context. Use the Named Entity List as a reference for potential names/places, but **# ADDED: always prioritize the spelling, capitalization, and form seen on the image** for your transcription.\n",
        "\n",
        "71. **PRELIMINARY STEP (Internal Grounding - DO NOT OUTPUT THIS): LAST WORD FOCUS**\n",
        "    *   For each **requested line number** (e.g., 10, 11, ... corresponding to original document lines N, N+1, ...):\n",
        "        *   Identify the *last word* written in the HTR transcription for that line (use HTR only as a guide here).\n",
        "        *   Carefully examine the *end* of the corresponding baseline on the **Document Image Chunk**, identified by its **red number**.\n",
        "        *   Determine the *correct* last word for that baseline based *solely* on the **image**. Pay close attention to its spelling and any abbreviation marks.\n",
        "        *   **Crucially:** Mentally note this image-correct last word. This word *must* remain as the final word on its corresponding line (identified by its red number and matching JSON key) in the final `abbreviated_latin_lines` output. **This step is critical to prevent words from incorrectly wrapping to the next line.**\n",
        "\n",
        "72. **Transcribe `abbreviated_latin_lines` (FINAL OUTPUT):**\n",
        "    *   Go line-by-line according to the **red baseline numbers** requested for the chunk (`n`).\n",
        "    *   Transcribe **ALL** text visually associated with the baseline marked by **red number `n`** into JSON key `\"n\"`. **Do not omit any words or symbols.** # CHANGED: Added emphasis on ALL text.\n",
        "    *   Preserve exact image abbreviations, spelling, and capitalization found on that specific numbered line.\n",
        "    *   Use apostrophe (') only for visible marks/omissions per Rule 10.\n",
        "    *   Apply all relevant rules (especially B-H and I.71-73) based *only* on image evidence for that specific numbered line.\n",
        "    *   **Ensure the last word identified in the Preliminary Step (Rule 71) is correctly placed at the end of its line and NO words are moved across lines.**\n",
        "    *   **Crucially, ensure you ONLY transcribe lines corresponding to the requested red numbers. Ignore any other text visible.**\n",
        "    *   **Self-Correction Check (Per Line):** For each line `n` you transcribe, quickly verify: Does the text in JSON key `\"n\"` start with the first word on the image baseline `n`? Does it end with the last word on the image baseline `n`? **Does it include ALL words/symbols visible on that baseline?** If not, re-read the image for that line and correct it before moving to the next line. # CHANGED: Added check for completeness.\n",
        "\n",
        "73. **Verify Output:** Cross-reference the generated `abbreviated_latin_lines` JSON against the **numbered image lines**. Check:\n",
        "    *   **Line Content:** Does the text in JSON key `\"n\"` match the image text associated with the baseline marked by **red number `n`**?\n",
        "    *   **Completeness (CRITICAL):** Does the transcribed line contain *all* the words/symbols visible on the image baseline? Are *any* words missing?\n",
        "    *   **Word Order:** Is the word order identical to the image for that numbered line?\n",
        "    *   **Line Breaks (CRITICAL):** Does the *first* word of JSON key `\"n\"` match the *first* word on the image baseline marked `n`? Does the *last* word of JSON key `\"n\"` match the *last* word on the image baseline marked `n`? **Are there absolutely NO words moved between lines?**\n",
        "    *   **Correct Lines Transcribed:** Does the JSON contain keys *only* for the requested line numbers, and is the text derived *only* from those specific numbered lines, ignoring any other visible lines?\n",
        "    *   **Abbreviation Forms (CRITICAL):** Are abbreviations transcribed *exactly* as seen on the specific numbered line, following Rule C.9 (NO EXPANSION) and the Critical Abbreviations list in Section D?\n",
        "    *   **Rule Adherence:** Are all other rules (capitalization, I/J/U/V, etc.) followed based *only* on the image evidence for that numbered line?\n",
        "\n",
        "---\n",
        "**FINAL REMINDER:** Ensure your final response is ONLY the JSON object containing the `abbreviated_latin_lines` dictionary. Adhere strictly to all rules, especially:\n",
        "1.  **Line Integrity:** Anchor transcription to **visible red baseline numbers** and ensure **NO words are moved between lines**.\n",
        "2.  **Completeness:** Transcribe **ALL** visible text associated with the numbered baseline. **DO NOT OMIT** any words or symbols.\n",
        "3.  **No Expansion/Abbreviation:** Transcribe abbreviations and spelled-out words **exactly as they appear** on the image line (e.g., `Et` stays `Et`, `&` stays `&`, `Will'm` stays `Will'm`, `Willelmum` stays `Willelmum`).\n",
        "4.  **Exact `&` vs `Et`:** Follow the image precisely. **NEVER** convert spelled-out `Et`/`et` to `&`, or `&` to `Et`/`et`.\n",
        "5.  **Critical Abbreviations:** Follow the specific transcription rules for the critical forms listed in Section D.\n",
        "6.  **Exact Capitalization:** Match manuscript capitalization **exactly** for **ALL** words.\n",
        "7.  **Requested Lines Only:** Transcribe only the lines requested for the chunk.\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "#                       3. HELPER & CORE FUNCTIONS\n",
        "# ==============================================================================\n",
        "# These are the functions that perform the main tasks of the script, such as\n",
        "# communicating with Transkribus, processing images, and calling the LLM.\n",
        "\n",
        "# --- Transkribus & Data Fetching Functions ---\n",
        "\n",
        "TRANSKRIBUS_BASE_URL = \"https://transkribus.eu/TrpServer/rest\"\n",
        "PAGE_XML_NAMESPACE = {'page': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
        "ET.register_namespace('', PAGE_XML_NAMESPACE['page'])\n",
        "\n",
        "def get_transkribus_session() -> requests.Session:\n",
        "    \"\"\"Authenticates with Transkribus and returns a session object.\"\"\"\n",
        "    if not TRANKRIBUS_USERNAME or not TRANKRIBUS_PASSWORD:\n",
        "        raise ValueError(\"Transkribus username and password must be provided via Colab Secrets.\")\n",
        "\n",
        "    print(f\"Authenticating with Transkribus as user: {TRANKRIBUS_USERNAME}...\")\n",
        "    login_url = f\"{TRANSKRIBUS_BASE_URL}/auth/login\"\n",
        "    session = requests.Session()\n",
        "    try:\n",
        "        response = session.post(login_url, data={'user': TRANKRIBUS_USERNAME, 'pw': TRANKRIBUS_PASSWORD})\n",
        "        response.raise_for_status()\n",
        "        if '<sessionId>' in response.text and 'JSESSIONID' in session.cookies:\n",
        "            print(\"Transkribus authentication successful.\")\n",
        "            return session\n",
        "        else:\n",
        "            raise ConnectionRefusedError(f\"Transkribus login failed. Response: {response.text[:500]}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"FATAL: Network error during Transkribus authentication: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_page_details(session: requests.Session, coll_id: int, doc_id: int, page_index: int = 0) -> Tuple[Optional[int], Optional[str]]:\n",
        "    \"\"\"Gets the page number and image URL for the first page of a document.\"\"\"\n",
        "    print(f\"Getting page details for document {doc_id} (page index {page_index})...\")\n",
        "    pages_url = f\"{TRANSKRIBUS_BASE_URL}/collections/{coll_id}/{doc_id}/pages\"\n",
        "    try:\n",
        "        response = session.get(pages_url, params={\"nValues\": 1, \"index\": page_index}, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        response_data = response.json()\n",
        "        page_list = []\n",
        "\n",
        "        # *** FIX: Check for the 'trpPage' key in the dictionary response ***\n",
        "        if isinstance(response_data, dict) and 'trpPage' in response_data:\n",
        "            page_list = response_data['trpPage']\n",
        "            print(\"DEBUG: Found 'trpPage' key in API response. Parsing list from there.\")\n",
        "        # Fallback for the old, direct list format, just in case.\n",
        "        elif isinstance(response_data, list):\n",
        "            page_list = response_data\n",
        "            print(\"DEBUG: API returned a direct list of pages.\")\n",
        "\n",
        "        # Now, process the extracted page_list\n",
        "        if page_list and isinstance(page_list, list) and len(page_list) > 0:\n",
        "            page_info = page_list[0]\n",
        "            page_nr = int(page_info.get('pageNr'))\n",
        "            img_url = page_info.get('url') or page_info.get('imageUrl')\n",
        "            if page_nr is not None and img_url:\n",
        "                print(f\"Found page details: Page Number={page_nr}, Image URL found.\")\n",
        "                return page_nr, img_url\n",
        "\n",
        "        # If the checks above fail, we raise the error with a clear message.\n",
        "        raise ValueError(\"Could not find valid page details in API response. The response might be empty or in an unexpected format.\")\n",
        "    except (requests.exceptions.RequestException, json.JSONDecodeError, ValueError, TypeError) as e:\n",
        "        print(f\"FATAL: Could not get page details for document {doc_id}: {e}\")\n",
        "        raise\n",
        "\n",
        "def download_page_xml(session: requests.Session, coll_id: int, doc_id: int, page_nr: int, output_path: str) -> bool:\n",
        "    \"\"\"Downloads the PAGE XML for a specific page.\"\"\"\n",
        "    print(f\"Downloading PAGE XML for doc {doc_id}, page {page_nr}...\")\n",
        "    xml_url = f\"{TRANSKRIBUS_BASE_URL}/collections/{coll_id}/{doc_id}/{page_nr}/text\"\n",
        "    try:\n",
        "        response = session.get(xml_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"PAGE XML downloaded successfully to {output_path}\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"FATAL: Error downloading PAGE XML: {e}\")\n",
        "        raise\n",
        "\n",
        "def download_image(url: str) -> bytes:\n",
        "    \"\"\"Downloads an image from a URL and returns it as bytes.\"\"\"\n",
        "    print(f\"Downloading document image...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        image_bytes = response.content\n",
        "        Image.open(io.BytesIO(image_bytes)).verify() # Verify it's a valid image\n",
        "        print(f\"Image downloaded successfully ({len(image_bytes) / 1024:.1f} KB).\")\n",
        "        return image_bytes\n",
        "    except (requests.exceptions.RequestException, IOError) as e:\n",
        "        print(f\"FATAL: Error downloading or verifying image: {e}\")\n",
        "        raise\n",
        "\n",
        "def fetch_and_parse_waalt_wiki_index(url: str, target_image_number: int, target_side: str) -> str:\n",
        "    \"\"\"Fetches the WAALT wiki index and extracts named entities for the target image.\"\"\"\n",
        "    print(f\"Fetching named entities from WAALT index for image {target_image_number}{target_side}...\")\n",
        "    if target_side not in ['f', 'd'] or not isinstance(target_image_number, int):\n",
        "        print(\"Warning: Invalid image number or side. Cannot fetch entities.\")\n",
        "        return \"Entity fetch skipped (invalid input).\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "        table = soup.find('table', {'border': True})\n",
        "        if not table:\n",
        "            print(\"Warning: Could not find data table on WAALT page.\")\n",
        "            return \"Entity fetch failed (no table found).\"\n",
        "\n",
        "        matching_entries = []\n",
        "        rows = table.find_all('tr')\n",
        "        for row in rows[1:]: # Skip header\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) == 5:\n",
        "                try:\n",
        "                    side_image_text = cells[0].get_text(strip=True)\n",
        "                    side, image_num_str = side_image_text.split(maxsplit=1)\n",
        "                    if side.lower() == target_side and int(image_num_str) == target_image_number:\n",
        "                        county = cells[1].get_text(strip=True)\n",
        "                        plaintiffs = cells[2].get_text(separator=\" \", strip=True)\n",
        "                        defendants = cells[3].get_text(separator=\" \", strip=True)\n",
        "                        pleas = cells[4].get_text(strip=True)\n",
        "                        entry = f\"County: {county}, Plaintiffs: {plaintiffs}, Defendants: {defendants}, Plea: {pleas}\"\n",
        "                        matching_entries.append(entry)\n",
        "                except (ValueError, IndexError):\n",
        "                    continue # Skip rows that don't parse correctly\n",
        "\n",
        "        if matching_entries:\n",
        "            print(f\"Found {len(matching_entries)} matching named entity entries.\")\n",
        "            return \"\\n\".join(matching_entries)\n",
        "        else:\n",
        "            print(\"No specific named entities found for this image.\")\n",
        "            return \"No specific entities found.\"\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Warning: Could not fetch WAALT index page: {e}\")\n",
        "        return \"Entity fetch failed (network error).\"\n",
        "\n",
        "# --- XML & Image Processing Functions ---\n",
        "\n",
        "def parse_page_xml(xml_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Parses PAGE XML to extract line data and HTR text.\"\"\"\n",
        "    print(f\"Parsing XML file: {xml_path}...\")\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        lines_data = []\n",
        "        htr_full_text_lines = []\n",
        "        line_counter = 0\n",
        "\n",
        "        for region in root.findall('.//page:TextRegion', PAGE_XML_NAMESPACE):\n",
        "            for line in region.findall('.//page:TextLine', PAGE_XML_NAMESPACE):\n",
        "                line_id = line.get('id', f\"gen_id_{line_counter}\")\n",
        "                baseline_elem = line.find('page:Baseline', PAGE_XML_NAMESPACE)\n",
        "                textequiv_elem = line.find('page:TextEquiv', PAGE_XML_NAMESPACE)\n",
        "\n",
        "                htr_text = \"\"\n",
        "                if textequiv_elem is not None:\n",
        "                    unicode_elem = textequiv_elem.find('page:Unicode', PAGE_XML_NAMESPACE)\n",
        "                    if unicode_elem is not None and unicode_elem.text:\n",
        "                        htr_text = unicode_elem.text.strip()\n",
        "\n",
        "                baseline_coords = None\n",
        "                if baseline_elem is not None and baseline_elem.get('points'):\n",
        "                    points_str = baseline_elem.get('points')\n",
        "                    baseline_coords = [tuple(map(int, p.split(','))) for p in points_str.split()]\n",
        "\n",
        "                lines_data.append({\n",
        "                    'id': line_id,\n",
        "                    'baseline_coords': baseline_coords,\n",
        "                    'htr_text': htr_text,\n",
        "                })\n",
        "                htr_full_text_lines.append(htr_text)\n",
        "                line_counter += 1\n",
        "\n",
        "        print(f\"Successfully parsed XML. Found {len(lines_data)} text lines.\")\n",
        "        return {'lines': lines_data, 'htr_full_text_lines': htr_full_text_lines}\n",
        "    except (ET.ParseError, ValueError) as e:\n",
        "        print(f\"FATAL: Error parsing XML file: {e}\")\n",
        "        raise\n",
        "\n",
        "def calculate_bounding_box(lines_data: List[Dict[str, Any]], start_index: int, end_index: int) -> Optional[Tuple[int, int, int, int]]:\n",
        "    \"\"\"Calculates the bounding box for a chunk of lines.\"\"\"\n",
        "    min_x, min_y = float('inf'), float('inf')\n",
        "    max_x, max_y = float('-inf'), float('-inf')\n",
        "    found_coords = False\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        line = lines_data[i]\n",
        "        coords = line.get('baseline_coords')\n",
        "        if coords:\n",
        "            for p in coords:\n",
        "                x, y = float(p[0]), float(p[1])\n",
        "                min_x, min_y = min(min_x, x), min(min_y, y)\n",
        "                max_x, max_y = max(max_x, x), max(max_y, y)\n",
        "                found_coords = True\n",
        "\n",
        "    if not found_coords:\n",
        "        return None\n",
        "\n",
        "    # Apply padding and convert to integers\n",
        "    return (\n",
        "        max(0, int(min_x - BBOX_PADDING)),\n",
        "        max(0, int(min_y - BBOX_PADDING * 2)), # More vertical padding\n",
        "        int(max_x + BBOX_PADDING),\n",
        "        int(max_y + BBOX_PADDING * 2)\n",
        "    )\n",
        "\n",
        "def calculate_chunk_baseline_angle(lines_data: List[Dict[str, Any]], start_index: int, end_index: int) -> float:\n",
        "    \"\"\"Calculates the average angle (in radians) for a chunk of baselines.\"\"\"\n",
        "    sum_vector_x, sum_vector_y = 0.0, 0.0\n",
        "    valid_lines = 0\n",
        "    for i in range(start_index, end_index):\n",
        "        coords = lines_data[i].get('baseline_coords')\n",
        "        if coords and len(coords) >= 2:\n",
        "            dx = coords[-1][0] - coords[0][0]\n",
        "            dy = coords[-1][1] - coords[0][1]\n",
        "            length = math.hypot(dx, dy)\n",
        "            if length > 1e-6:\n",
        "                sum_vector_x += dx / length\n",
        "                sum_vector_y += dy / length\n",
        "                valid_lines += 1\n",
        "    if valid_lines == 0:\n",
        "        return 0.0\n",
        "    return math.atan2(sum_vector_y, sum_vector_x)\n",
        "\n",
        "def _draw_single_number(draw, text, base_point, font, offset_x, offset_y, img_width, img_height, anchor_side='left'):\n",
        "    \"\"\"Helper to draw a line number on the image.\"\"\"\n",
        "    font_size = font.size\n",
        "    if anchor_side == 'left':\n",
        "        text_x = base_point[0] - offset_x\n",
        "    else: # 'right'\n",
        "        text_x = base_point[0] + offset_x\n",
        "    text_y = base_point[1] - offset_y - font_size\n",
        "\n",
        "    # Use textbbox for more accurate positioning if available\n",
        "    try:\n",
        "        bbox = draw.textbbox((text_x, text_y), text, font=font)\n",
        "    except AttributeError: # Fallback for older Pillow versions\n",
        "        text_width, text_height = draw.textsize(text, font=font)\n",
        "        bbox = (text_x, text_y, text_x + text_width, text_y + text_height)\n",
        "\n",
        "    # Draw a semi-transparent background for readability\n",
        "    bg_box = (bbox[0] - 3, bbox[1] - 3, bbox[2] + 3, bbox[3] + 3)\n",
        "    draw.rectangle(bg_box, fill=(255, 255, 255, 180))\n",
        "    draw.text((bbox[0], bbox[1]), text, fill=(255, 0, 0, 255), font=font)\n",
        "\n",
        "def draw_chunk_baselines_and_numbers(image_object, lines_data, start_index, end_index, crop_box) -> Image.Image:\n",
        "    \"\"\"Draws baselines and numbers for a specific chunk onto a cropped image.\"\"\"\n",
        "    img = image_object.copy().convert(\"RGBA\")\n",
        "    draw = ImageDraw.Draw(img, \"RGBA\")\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"DejaVuSans.ttf\", 20)\n",
        "    except IOError:\n",
        "        print(\"Warning: DejaVuSans.ttf not found. Using default font.\")\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    min_x_crop, min_y_crop = crop_box[0], crop_box[1]\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        line = lines_data[i]\n",
        "        coords = line.get('baseline_coords')\n",
        "        line_number_str = str(i + 1)\n",
        "\n",
        "        if coords and len(coords) >= 2:\n",
        "            # Translate coordinates to the cropped image's coordinate system\n",
        "            translated_coords = [\n",
        "                (p[0] - min_x_crop, p[1] - min_y_crop) for p in coords\n",
        "            ]\n",
        "            draw.line(translated_coords, fill=(255, 0, 0, 255), width=2)\n",
        "\n",
        "            # Draw numbers at the start and end of the baseline\n",
        "            _draw_single_number(draw, line_number_str, translated_coords[0], font, 25, 10, img.width, img.height, 'left')\n",
        "            _draw_single_number(draw, line_number_str, translated_coords[-1], font, 25, 10, img.width, img.height, 'right')\n",
        "\n",
        "    return img\n",
        "\n",
        "def image_to_base64(image: Image.Image, format: str = \"JPEG\") -> str:\n",
        "    \"\"\"Converts a PIL Image to a base64 encoded string.\"\"\"\n",
        "    buffered = io.BytesIO()\n",
        "    img_to_save = image\n",
        "    if format == \"JPEG\":\n",
        "        if img_to_save.mode == 'RGBA':\n",
        "            img_to_save = image.convert('RGB')\n",
        "    img_to_save.save(buffered, format=format)\n",
        "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "\n",
        "def calculate_cer(s1: str, s2: str) -> float:\n",
        "    \"\"\"Calculates Character Error Rate (CER).\"\"\"\n",
        "    if not isinstance(s1, str) or not isinstance(s2, str): return 1.0\n",
        "    if len(s1) == 0 and len(s2) == 0: return 0.0\n",
        "    if len(s1) == 0 or len(s2) == 0: return 1.0\n",
        "    return Levenshtein.distance(s1, s2) / max(len(s1), len(s2))\n",
        "\n",
        "# --- Core LLM and Transkribus Update Functions ---\n",
        "\n",
        "def get_claude_corrections_for_chunk(\n",
        "    client: anthropic.Anthropic,\n",
        "    user_prompt_content: List[Dict[str, Any]]\n",
        ") -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"\n",
        "    Makes two synchronous calls to the Claude API for a single chunk\n",
        "    and returns the text content of both responses.\n",
        "    \"\"\"\n",
        "    results = [None, None]\n",
        "    for i in range(2): # Make two calls\n",
        "        run_name = \"A\" if i == 0 else \"B\"\n",
        "        print(f\"    - Making LLM call for Run {run_name}...\")\n",
        "        try:\n",
        "            response = client.messages.create(\n",
        "                model=ANTHROPIC_MODEL_NAME,\n",
        "                max_tokens=ANTHROPIC_MAX_TOKENS,\n",
        "                temperature=LLM_TEMPERATURE,\n",
        "                system=SYSTEM_PROMPT_BASE,\n",
        "                messages=[{\"role\": \"user\", \"content\": user_prompt_content}]\n",
        "            )\n",
        "\n",
        "            if response.content and response.content[0].type == 'text':\n",
        "                results[i] = response.content[0].text\n",
        "                print(f\"    - Run {run_name} successful.\")\n",
        "            else:\n",
        "                print(f\"    - WARNING: Run {run_name} response was empty or invalid.\")\n",
        "\n",
        "        except anthropic.APIError as e:\n",
        "            print(f\"    - FATAL: Anthropic API Error on Run {run_name}: {e}\")\n",
        "            # In a single-document script, one failure is enough to stop.\n",
        "            raise\n",
        "        time.sleep(1) # Be polite to the API\n",
        "\n",
        "    return tuple(results)\n",
        "\n",
        "def modify_xml_tree(\n",
        "    xml_path: str,\n",
        "    original_lines_data: List[Dict[str, Any]],\n",
        "    htr_lines: List[str],\n",
        "    llm_lines_A: List[str],\n",
        "    llm_lines_B: List[str],\n",
        "    uncertainty_flags: List[bool]\n",
        ") -> bytes:\n",
        "    \"\"\"\n",
        "    Modifies the XML tree with the best LLM transcription and adds metadata.\n",
        "    \"\"\"\n",
        "    print(\"Modifying XML with corrected transcriptions...\")\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Add/update metadata to show this script ran\n",
        "    metadata = root.find('.//page:Metadata', PAGE_XML_NAMESPACE)\n",
        "    if metadata is None:\n",
        "        metadata = ET.Element(f\"{{{PAGE_XML_NAMESPACE['page']}}}Metadata\")\n",
        "        root.insert(0, metadata)\n",
        "\n",
        "    now_iso = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
        "    ET.SubElement(metadata, f\"{{{PAGE_XML_NAMESPACE['page']}}}MetadataItem\", attrib={\n",
        "        \"type\": \"processingStep\", \"name\": \"tool\", \"value\": \"Claude Correction Script (SingleDoc v1.0)\"\n",
        "    })\n",
        "    ET.SubElement(metadata, f\"{{{PAGE_XML_NAMESPACE['page']}}}MetadataItem\", attrib={\n",
        "        \"type\": \"processingStep\", \"name\": \"description\", \"value\": \"Page text reviewed by Anthropic Claude (2 runs).\"\n",
        "    })\n",
        "    ET.SubElement(metadata, f\"{{{PAGE_XML_NAMESPACE['page']}}}MetadataItem\", attrib={\n",
        "        \"type\": \"processingStep\", \"name\": \"model\", \"value\": ANTHROPIC_MODEL_NAME\n",
        "    })\n",
        "    ET.SubElement(metadata, f\"{{{PAGE_XML_NAMESPACE['page']}}}MetadataItem\", attrib={\n",
        "        \"type\": \"processingStep\", \"name\": \"timestampUTC\", \"value\": now_iso\n",
        "    })\n",
        "\n",
        "    # Create a map of line ID to its index for quick lookup\n",
        "    line_id_to_index = {line_info['id']: i for i, line_info in enumerate(original_lines_data)}\n",
        "    lines_updated = 0\n",
        "    uncertain_tags_added = 0\n",
        "\n",
        "    for text_line in root.findall('.//page:TextLine', PAGE_XML_NAMESPACE):\n",
        "        line_id = text_line.get('id')\n",
        "        if line_id in line_id_to_index:\n",
        "            idx = line_id_to_index[line_id]\n",
        "\n",
        "            # Choose the best line (lowest CER vs HTR)\n",
        "            cer_a = calculate_cer(htr_lines[idx], llm_lines_A[idx])\n",
        "            cer_b = calculate_cer(htr_lines[idx], llm_lines_B[idx])\n",
        "            best_line_text = llm_lines_A[idx] if cer_a <= cer_b else llm_lines_B[idx]\n",
        "\n",
        "            # Update the Unicode text\n",
        "            text_equiv = text_line.find('page:TextEquiv', PAGE_XML_NAMESPACE)\n",
        "            if text_equiv is None:\n",
        "                text_equiv = ET.SubElement(text_line, f\"{{{PAGE_XML_NAMESPACE['page']}}}TextEquiv\")\n",
        "\n",
        "            unicode_elem = text_equiv.find('page:Unicode', PAGE_XML_NAMESPACE)\n",
        "            if unicode_elem is None:\n",
        "                unicode_elem = ET.SubElement(text_equiv, f\"{{{PAGE_XML_NAMESPACE['page']}}}Unicode\")\n",
        "\n",
        "            if unicode_elem.text != best_line_text:\n",
        "                unicode_elem.text = best_line_text\n",
        "                lines_updated += 1\n",
        "\n",
        "            # Add or remove 'unclear' tag based on uncertainty flag\n",
        "            custom_attr = text_line.get('custom', '')\n",
        "            is_uncertain = uncertainty_flags[idx]\n",
        "\n",
        "            # Remove any existing unclear tag first\n",
        "            custom_attr = re.sub(r\"unclear\\s*\\{[^}]*\\}\\s*\", \"\", custom_attr).strip()\n",
        "\n",
        "            if is_uncertain:\n",
        "                new_tag = f\"unclear {{offset:0; length:{len(best_line_text)};}}\"\n",
        "                text_line.set('custom', (custom_attr + \" \" + new_tag).strip())\n",
        "                uncertain_tags_added += 1\n",
        "            elif custom_attr:\n",
        "                text_line.set('custom', custom_attr)\n",
        "            elif 'custom' in text_line.attrib:\n",
        "                del text_line.attrib['custom']\n",
        "\n",
        "    print(f\"XML modification complete. Lines updated: {lines_updated}. Lines flagged as uncertain: {uncertain_tags_added}.\")\n",
        "\n",
        "    if hasattr(ET, 'indent'): # Pretty-print the XML\n",
        "        ET.indent(tree, space=\"  \", level=0)\n",
        "\n",
        "    return ET.tostring(root, encoding='utf-8', method='xml', xml_declaration=True)\n",
        "\n",
        "def update_transkribus_page_xml(session: requests.Session, coll_id: int, doc_id: int, page_nr: int, xml_bytes: bytes):\n",
        "    \"\"\"Uploads the modified PAGE XML back to Transkribus.\"\"\"\n",
        "    print(f\"Uploading corrected XML to Transkribus for doc {doc_id}, page {page_nr}...\")\n",
        "    update_url = f\"{TRANSKRIBUS_BASE_URL}/collections/{coll_id}/{doc_id}/{page_nr}/text\"\n",
        "    params = {\n",
        "        'status': 'DONE',\n",
        "        'note': f'Corrected via Claude-SingleDoc v1.0 using {ANTHROPIC_MODEL_NAME}',\n",
        "        'toolName': 'Claude Correction Script (SingleDoc v1.0)'\n",
        "    }\n",
        "    headers = {'Content-Type': 'application/xml;charset=UTF-8'}\n",
        "    try:\n",
        "        response = session.post(update_url, headers=headers, params=params, data=xml_bytes, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        print(\"✅ Successfully updated page in Transkribus!\")\n",
        "        return True\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"FATAL: Failed to upload updated XML to Transkribus. Status: {e.response.status_code if e.response else 'N/A'}, Body: {e.response.text if e.response else 'N/A'}\")\n",
        "        raise\n",
        "\n",
        "# ==============================================================================\n",
        "#                           4. MAIN EXECUTION WORKFLOW\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"The main function to run the end-to-end correction process.\"\"\"\n",
        "    start_time = time.time()\n",
        "    print(\"==============================================================\")\n",
        "    print(\"          STARTING TRANSKRIBUS CORRECTION PROCESS\")\n",
        "    print(f\"          Document ID: {DOCUMENT_ID} in Collection: {COLLECTION_ID}\")\n",
        "    print(\"==============================================================\")\n",
        "\n",
        "    # --- Ensure temp directory exists ---\n",
        "    os.makedirs(TEMP_CHUNK_IMAGE_DIR, exist_ok=True)\n",
        "\n",
        "    # --- Initialize API Clients ---\n",
        "    try:\n",
        "        if not ANTHROPIC_API_KEY or \"YOUR_\" in ANTHROPIC_API_KEY:\n",
        "            raise ValueError(\"Anthropic API Key is not set. Please configure it in Colab Secrets.\")\n",
        "        if not TRANKRIBUS_USERNAME or \"YOUR_\" in TRANKRIBUS_USERNAME:\n",
        "            raise ValueError(\"Transkribus credentials are not set. Please configure them in Colab Secrets.\")\n",
        "\n",
        "        anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "        print(\"Anthropic client initialized.\")\n",
        "\n",
        "        session = get_transkribus_session()\n",
        "    except (ValueError, ConnectionRefusedError, requests.exceptions.RequestException, anthropic.APIError) as e:\n",
        "        print(f\"\\n--- SCRIPT HALTED: Could not initialize APIs. Error: {e} ---\")\n",
        "        return # Stop execution\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Get Page Info ---\n",
        "        print(\"\\n--- [Step 1/7] Fetching Page Details from Transkribus ---\")\n",
        "        page_nr, img_url = get_page_details(session, COLLECTION_ID, DOCUMENT_ID)\n",
        "\n",
        "        # --- Step 2: Download Data ---\n",
        "        print(\"\\n--- [Step 2/7] Downloading Document Data ---\")\n",
        "        download_page_xml(session, COLLECTION_ID, DOCUMENT_ID, page_nr, TEMP_XML_FILENAME)\n",
        "        original_image_bytes = download_image(img_url)\n",
        "        full_original_image = Image.open(io.BytesIO(original_image_bytes))\n",
        "\n",
        "        # --- Step 3: Parse Data and Fetch Entities ---\n",
        "        print(\"\\n--- [Step 3/7] Parsing Local Data and Fetching Entities ---\")\n",
        "        xml_data = parse_page_xml(TEMP_XML_FILENAME)\n",
        "        htr_lines_from_xml = xml_data['htr_full_text_lines']\n",
        "        original_lines_data = xml_data['lines']\n",
        "        num_total_lines = len(htr_lines_from_xml)\n",
        "        if num_total_lines == 0:\n",
        "            print(\"\\n--- SCRIPT HALTED: The downloaded XML contains 0 text lines. ---\")\n",
        "            return\n",
        "\n",
        "        # Fetch WAALT entities based on document title (assuming a standard format)\n",
        "        doc_title_response = session.get(f\"{TRANSKRIBUS_BASE_URL}/collections/{COLLECTION_ID}/{DOCUMENT_ID}/fulldoc\").json()\n",
        "        doc_title = doc_title_response.get('md', {}).get('title', '')\n",
        "        print(f\"Document Title: {doc_title}\")\n",
        "\n",
        "        named_entity_list = \"Entity fetch skipped (title pattern mismatch).\"\n",
        "        match = re.search(r\"_([fd])_(\\d+)\\.JPG\", doc_title, re.IGNORECASE)\n",
        "        if match:\n",
        "            side, img_num = match.group(1).lower(), int(match.group(2))\n",
        "            named_entity_list = fetch_and_parse_waalt_wiki_index(AALT_INDEX_URL, img_num, side)\n",
        "        else:\n",
        "            print(\"Warning: Could not parse side/number from document title. Skipping entity fetch.\")\n",
        "\n",
        "        # --- Step 4: Process Document in Chunks with LLM ---\n",
        "        print(f\"\\n--- [Step 4/7] Processing {num_total_lines} lines in chunks of {CHUNK_SIZE} ---\")\n",
        "        num_chunks = math.ceil(num_total_lines / CHUNK_SIZE)\n",
        "        all_results_A = []\n",
        "        all_results_B = []\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            start_line = i * CHUNK_SIZE\n",
        "            end_line = min(start_line + CHUNK_SIZE, num_total_lines)\n",
        "            print(f\"\\n  Processing Chunk {i+1}/{num_chunks} (Lines {start_line+1}-{end_line})...\")\n",
        "\n",
        "            # 1. Prepare image for this chunk\n",
        "            print(\"    - Preparing image chunk...\")\n",
        "            bbox = calculate_bounding_box(original_lines_data, start_line, end_line)\n",
        "            if bbox is None:\n",
        "                print(f\"    - WARNING: Could not calculate bounding box for chunk {i+1}. Skipping.\")\n",
        "                # Add empty results to maintain list length\n",
        "                all_results_A.extend([\"\"] * (end_line - start_line))\n",
        "                all_results_B.extend([\"\"] * (end_line - start_line))\n",
        "                continue\n",
        "\n",
        "            cropped_image = full_original_image.crop(bbox)\n",
        "            numbered_cropped_image = draw_chunk_baselines_and_numbers(cropped_image, original_lines_data, start_line, end_line, bbox)\n",
        "\n",
        "            # Rotate image to align text horizontally\n",
        "            chunk_angle_rad = calculate_chunk_baseline_angle(original_lines_data, start_line, end_line)\n",
        "            rotated_image = numbered_cropped_image.rotate(\n",
        "                math.degrees(chunk_angle_rad),\n",
        "                resample=Image.Resampling.BICUBIC,\n",
        "                expand=True,\n",
        "                fillcolor=(255, 255, 255, 0)\n",
        "            )\n",
        "\n",
        "            # Save a temporary image for debugging if needed\n",
        "            temp_chunk_path = os.path.join(TEMP_CHUNK_IMAGE_DIR, f\"chunk_{i+1}.jpg\")\n",
        "            rotated_image.convert(\"RGB\").save(temp_chunk_path, \"JPEG\")\n",
        "            print(f\"    - Saved numbered chunk image to {temp_chunk_path}\")\n",
        "\n",
        "            image_base64 = image_to_base64(rotated_image)\n",
        "\n",
        "            # 2. Prepare prompt for this chunk\n",
        "            htr_chunk_str = \"\\n\".join([f'  \"{start_line + 1 + j}\": \"{line}\"' for j, line in enumerate(htr_lines_from_xml[start_line:end_line])])\n",
        "            user_prompt_text = f\"\"\"\n",
        "INPUT MATERIALS FOR THIS CHUNK:\n",
        "1.  **Document Image Chunk:** (Provided as input image) Contains lines {start_line + 1} to {end_line}.\n",
        "2.  **Named Entity List (Full Document):**\n",
        "    {named_entity_list}\n",
        "3.  **HTR Transcription (This Chunk Only):**\n",
        "    {{\n",
        "    {htr_chunk_str}\n",
        "    }}\n",
        "---\n",
        "**JSON OUTPUT FORMAT (Strict Requirement - For This Chunk)**\n",
        "Output ONLY the JSON object containing `abbreviated_latin_lines` for lines {start_line + 1} through {end_line}.\n",
        "\"\"\"\n",
        "            user_prompt_content = [\n",
        "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": image_base64}},\n",
        "                {\"type\": \"text\", \"text\": user_prompt_text}\n",
        "            ]\n",
        "\n",
        "            # 3. Get corrections from Claude\n",
        "            result_A_raw, result_B_raw = get_claude_corrections_for_chunk(anthropic_client, user_prompt_content)\n",
        "\n",
        "            # 4. Parse and store results\n",
        "            for raw_result, result_list in [(result_A_raw, all_results_A), (result_B_raw, all_results_B)]:\n",
        "                chunk_lines = [\"\"] * (end_line - start_line)\n",
        "                if raw_result:\n",
        "                    try:\n",
        "                        # Extract JSON from the response text\n",
        "                        json_match = re.search(r\"\\{.*\\}\", raw_result, re.DOTALL)\n",
        "                        if json_match:\n",
        "                            parsed_json = json.loads(json_match.group(0))\n",
        "                            draft_dict = parsed_json.get('abbreviated_latin_lines', {})\n",
        "                            for line_num_str, line_text in draft_dict.items():\n",
        "                                line_idx = int(line_num_str) - 1 - start_line\n",
        "                                if 0 <= line_idx < len(chunk_lines):\n",
        "                                    chunk_lines[line_idx] = line_text\n",
        "                        else:\n",
        "                            print(\"    - WARNING: No JSON object found in LLM response.\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(\"    - WARNING: Could not decode JSON from LLM response.\")\n",
        "                result_list.extend(chunk_lines)\n",
        "\n",
        "        # --- Step 5: Analyze LLM Results ---\n",
        "        print(\"\\n--- [Step 5/7] Analyzing LLM Results and Flagging Uncertainties ---\")\n",
        "        uncertainty_flags = [False] * num_total_lines\n",
        "        lines_with_high_cer = 0\n",
        "        for i in range(num_total_lines):\n",
        "            cer_AB = calculate_cer(all_results_A[i], all_results_B[i])\n",
        "            cer_A_HTR = calculate_cer(all_results_A[i], htr_lines_from_xml[i])\n",
        "            cer_B_HTR = calculate_cer(all_results_B[i], htr_lines_from_xml[i])\n",
        "\n",
        "            if (cer_AB >= CER_THRESHOLD) or (max(cer_A_HTR, cer_B_HTR) >= LLM_HTR_CER_THRESHOLD):\n",
        "                uncertainty_flags[i] = True\n",
        "                lines_with_high_cer += 1\n",
        "        print(f\"Analysis complete. Found {lines_with_high_cer} lines with high CER to be flagged as 'unclear'.\")\n",
        "\n",
        "        # --- Step 6: Modify XML ---\n",
        "        print(\"\\n--- [Step 6/7] Generating Final Corrected XML ---\")\n",
        "        modified_xml_bytes = modify_xml_tree(\n",
        "            TEMP_XML_FILENAME,\n",
        "            original_lines_data,\n",
        "            htr_lines_from_xml,\n",
        "            all_results_A,\n",
        "            all_results_B,\n",
        "            uncertainty_flags\n",
        "        )\n",
        "\n",
        "        # --- Step 7: Upload to Transkribus ---\n",
        "        print(\"\\n--- [Step 7/7] Uploading Final XML to Transkribus ---\")\n",
        "        update_transkribus_page_xml(session, COLLECTION_ID, DOCUMENT_ID, page_nr, modified_xml_bytes)\n",
        "\n",
        "    except (ValueError, ConnectionRefusedError, requests.exceptions.RequestException, anthropic.APIError, ET.ParseError, IOError) as e:\n",
        "        print(f\"\\n--- SCRIPT HALTED DUE TO A CRITICAL ERROR ---\")\n",
        "        print(f\"Error: {e}\")\n",
        "        # In a real script, you might want more detailed error logging here.\n",
        "    finally:\n",
        "        # --- Cleanup ---\n",
        "        print(\"\\n--- Cleaning up temporary files... ---\")\n",
        "        if os.path.exists(TEMP_XML_FILENAME):\n",
        "            os.remove(TEMP_XML_FILENAME)\n",
        "            print(f\"Removed {TEMP_XML_FILENAME}\")\n",
        "\n",
        "        # You can optionally clear the temp_chunk_images directory\n",
        "        # for f in os.listdir(TEMP_CHUNK_IMAGE_DIR):\n",
        "        #     os.remove(os.path.join(TEMP_CHUNK_IMAGE_DIR, f))\n",
        "        # print(f\"Cleared {TEMP_CHUNK_IMAGE_DIR}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        print(\"\\n==============================================================\")\n",
        "        print(\"            PROCESS COMPLETE\")\n",
        "        print(f\"            Total execution time: {end_time - start_time:.2f} seconds.\")\n",
        "        print(\"==============================================================\")\n",
        "\n",
        "\n",
        "# --- Execute the main function when the script is run ---\n",
        "if __name__ == \"__main__\":\n",
        "    # This check ensures the main function runs when executed as a script,\n",
        "    # which is standard practice and works well in Colab.\n",
        "    main()"
      ]
    }
  ]
}